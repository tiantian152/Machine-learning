## 单变量线性回归

### less function

loss function通常用于衡量**单个样本**其预测值和实际值的“差距”

既然loss function是用于衡量预测值和实际值之间的"差距"，那么我们其实有很多的衡量手段，比如通过方差，如下：

![img](https:////upload-images.jianshu.io/upload_images/3959253-c576b993f32bb9a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/274/format/webp)

但是，在logistic regression算法中使用方差的方式无法得到凸函数(convex)，也就无法得到全局最小值，所以，我们在logistic regression中使用不同的loss function，如下：


![img](https:////upload-images.jianshu.io/upload_images/3959253-09a8e2ed2496a190.png?imageMogr2/auto-orient/strip|imageView2/2/w/444/format/webp)

 需要注意的是，不论是哪个函数，都是针对单个样本的，所以都带有上标 (i)



### cost function

cost function通常是针对**样本集中的所有样本**，而且是一个平均值。

cost function 是针对整个样本集的，因此它的计算公式需要将所有的loss function的结果进行加总然后求平均值，如下：

![img](https://upload-images.jianshu.io/upload_images/3959253-ca434a19993665f9.png?imageMogr2/auto-orient/strip|imageView2/2/w/653/format/webp)

使用如下

![cost function](F:\workspace\python\机器学习\吴恩达\单变量线性回归\cost function.PNG)

### 代码实现

```python
import random
import matplotlib.pyplot as plt
import numpy as np

# 生产训练数据集
n = 100                # 数据集元素个数
train_step = 1000       # 训练次数
learn_rate = 0.00005     # 学习效率
w = 2
b = 10
# y = 2x + 10
x_data = []
y_data = []
for i in range(n):
    x_data.append(i)    # x是顺序的从1到1000
    noise = random.randint(-20, 20)   # 生成一个大小在0-20之间的噪音
    y_data.append(x_data[i] * w + b + noise)    # 生成训练集中的y参数

# 训练线性回归模型
w_ = 0     # 初始化权重值
b_ = 0

for i in range(train_step):     # 训练权重值（重复train_step次）
    temp_w = 0
    temp_b = 0  
    for j in range(n):  #每次训练有n个数据
        y = x_data[j] * w_ + b_  # 计算出y的实际值
        loss = y - y_data[j]    # 计算损失值        
        temp_w += x_data[j] * loss   
        temp_b += loss
    
    w_ -= temp_w / n * learn_rate           # 更新权重值
    b_ -= temp_b / n * learn_rate * 10000

    if i % (train_step/10) == 0:    #分成十份打印实际权重值
        print(w_)
        print(b_)


# 测试回归模型正确度（图形化显示）
plt.scatter(x_data, y_data) # 打印训练集点数据
for i in range(n):
    y_data[i] = x_data[i] * w_ + b_
plt.plot(x_data, y_data,'r') # 打印实际线性回归值
plt.show()
```

