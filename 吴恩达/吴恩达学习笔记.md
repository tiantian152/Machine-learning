[TOC]



## 一、 引言

### 1.1 欢迎
### 1.2 机器学习是什么？

1. 介绍了机器学习初步的概念——一个不精通下棋的人写了个程序，程序自己和自己下了万把棋，记录下每把的经验最后该程序的棋技超过了编程者。

2. 最近的定义——一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。

### 1.3 监督学习

监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。

其实就是有标签的训练集，通过某种算法达成最好的拟合效果

- 回归问题（连续的）
- 分类问题（离散的）

### 1.4 无监督学习

现实生活中常常会有这样的问题：

（1）缺乏足够的先验知识，因此难以人工标注类别;

（2）进行人工类别标注的成本太高。

很自然地，我们希望计算机能代我们(部分)完成这些工作，或至少提供一些帮助。常见的应用背景包括：

（1）一从庞大的样本集合中选出一些具有代表性的加以标注用于分类器的训练。

（2）先将所有样本自动分为不同的类别，再由人类对这些类别进行标注。

（3）在无类别信息情况下，寻找好的特征。

这样的操作，其实就是无监督学习

- 通常使用聚类的方法来实现

## 二、单变量线性回归
### 2.1 模型表示

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps1.jpg) 代表训练集中实例的数量  

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps2.jpg) 代表特征/输入变量

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps3.jpg) 代表目标变量/输出变量

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps4.jpg) 代表训练集中的实例

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps5.jpg) 代表第![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps6.jpg) 个观察实例

![img](file:///C:\Users\天天\AppData\Local\Temp\ksohtml3508\wps7.jpg) 代表学习算法的解决方案或函数也称为假设（**hypothesis**）

### 2.2 代价函数

![1580647709467](C:\Users\天天\AppData\Roaming\Typora\typora-user-images\1580647709467.png)

#### less function

loss function通常用于衡量**单个样本**其预测值和实际值的“差距”

既然loss function是用于衡量预测值和实际值之间的"差距"，那么我们其实有很多的衡量手段，比如通过方差，如下：

![img](https:////upload-images.jianshu.io/upload_images/3959253-c576b993f32bb9a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/274/format/webp)

但是，在logistic regression算法中使用方差的方式无法得到凸函数(convex)，也就无法得到全局最小值，所以，我们在logistic regression中使用不同的loss function，如下：



![img](https:////upload-images.jianshu.io/upload_images/3959253-09a8e2ed2496a190.png?imageMogr2/auto-orient/strip|imageView2/2/w/444/format/webp)

 需要注意的是，不论是哪个函数，都是针对单个样本的，所以都带有上标 (i)



#### cost function

cost function通常是针对**样本集中的所有样本**，而且是一个平均值。

cost function 是针对整个样本集的，因此它的计算公式需要将所有的loss function的结果进行加总然后求平均值，如下：

![img](https://upload-images.jianshu.io/upload_images/3959253-ca434a19993665f9.png?imageMogr2/auto-orient/strip|imageView2/2/w/653/format/webp)

#### 优化目标

得到最小的cost function值

### 2.3 代价函数的直观理解I

只有一个参数时

![cost function](F:\workspace\python\机器学习\吴恩达\cost function.PNG)

即优化目标为1

### 2.4 代价函数的直观理解II

是一个两个参数的实例，实际上与 一个参数的类似，但是生成的cost function会多出一维（这很容易理解）（即变成三维曲面）

![1580646267317](F:\workspace\python\机器学习\吴恩达\1580646267317.png)

同样的，我们需要做的事情就是找出三维曲线中最小的值作为优化目标，当然，这个优化目标为二维的

### 2.5 梯度下降

那么我们如何找出上面说的优化目标呢，用到的就是梯度下降算法

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出**代价函数的最小值**。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

![1580647988012](C:\Users\天天\AppData\Roaming\Typora\typora-user-images\1580647988012.png)

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。（不同的起点可能会导致不同的结果）



批量梯度下降（**batch gradient descent**）算法的定义：

![1580648675200](C:\Users\天天\AppData\Roaming\Typora\typora-user-images\1580648675200.png)

其中 α 是学习率（**learning rate**），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

实现梯度下降算法的微妙之处是所有参数需要同时更新，具体步骤如图。

一般来说先将参数都初始化成0

### 2.6 梯度下降的直观理解

![IMG_0945(20200202-212622)](F:\workspace\python\机器学习\吴恩达\IMG_0945(20200202-212622).PNG)

如果 α 太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果 α 太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。

如果 α 太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α 太大，它会导致无法收敛，甚至发散。

如果将初始值放置在最低点，那么参数每次的变化将为0，但此时仍然进行着操作

在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，这时数值变化的也会变慢

### 2.7 梯度下降的线性回归

### 2.8 接下来的内容
## 三、线性代数回顾
### 3.1 矩阵和向量
### 3.2 加法和标量乘法
### 3.3 矩阵向量乘法
### 3.4 矩阵乘法
### 3.5 矩阵乘法的性质
### 3.6 逆、转置
## 四、多变量线性回归
### 4.1 多维特征
### 4.2 多变量梯度下降
### 4.3 梯度下降法实践1-特征缩放
### 4.4 梯度下降法实践2-学习率
### 4.5 特征和多项式回归
### 4.6 正规方程
### 4.7 正规方程及不可逆性（选修）
## 五、Octave教程(Octave Tutorial)
### 5.1 基本操作
### 5.2 移动数据
### 5.3 计算数据
### 5.4 绘图数据
### 5.5 控制语句：for，while，if语句
### 5.6 向量化 88
### 5.7 工作和提交的编程练习
## 六、逻辑回归
### 6.1 分类问题
### 6.2 假说表示
### 6.3 判定边界
### 6.4 代价函数
### 6.5 简化的成本函数和梯度下降
### 6.6 高级优化
### 6.7 多类别分类：一对多
## 七、正则化
### 7.1 过拟合的问题
### 7.2 代价函数
### 7.3 正则化线性回归
### 7.4 正则化的逻辑回归模型
## 第八、神经网络：表述
### 8.1 非线性假设
### 8.2 神经元和大脑
### 8.3 模型表示1
### 8.4 模型表示2
### 8.5 样本和直观理解1
### 8.6 样本和直观理解II
### 8.7 多类分类

九、神经网络的学习
9.1 代价函数
9.2 反向传播算法
9.3 反向传播算法的直观理解
9.4 实现注意：展开参数
9.5 梯度检验
9.6 随机初始化
9.7 综合起来
9.8 自主驾驶
十、应用机器学习的建议
10.1 决定下一步做什么
10.2 评估一个假设
10.3 模型选择和交叉验证集
10.4 诊断偏差和方差
10.5 正则化和偏差/方差
10.6 学习曲线
10.7 决定下一步做什么
十一、机器学习系统的设计
11.1 首先要做什么
11.2 误差分析
11.3 类偏斜的误差度量
11.4 查准率和查全率之间的权衡
11.5 机器学习的数据
十二、支持向量机
12.1 优化目标
12.2 大边界的直观理解
12.3 数学背后的大边界分类
12.4 核函数1
12.5 核函数2
12.6 使用支持向量机
十三、聚类
13.1 无监督学习：简介
13.2 K-均值算法
13.3 优化目标
13.4 随机初始化
13.5 选择聚类数
十四、降唯
14.1 动机一：数据压缩
14.2 动机二：数据可视化
14.3 主成分分析问题
14.4 主成分分析算法
14.5 选择主成分的数量
14.6 重建的压缩表示
14.7 主成分分析法的应用建议
十五、异常检测
15.1 问题的动机
15.2 高斯分布
15.3 算法
15.4 开发和评价一个异常检测系统
15.5 异常检测与监督学习对比
15.6 选择特征
15.7 多元高斯分布
15.8 使用多元高斯分布进行异常检测
十六、推荐系统
16.1 问题形式化
16.2 基于内容的推荐系统
16.3 协同过滤
16.4 协同过滤算法
16.5 向量化